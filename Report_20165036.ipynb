{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Report_20165036.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP98WUHLz7PS7L4WVIZjAOS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YeojinKim220/DL_Project_Micrograph_Colorization/blob/master/Report_20165036.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Nv5dKOHCYcF",
        "colab_type": "text"
      },
      "source": [
        "# Colorizing Micrograph\n",
        "\n",
        "*CNN을 이용한 전자현미경 이미지 채색*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf-elNMzA5nA",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Introduction\n",
        "뉴스나 신문에서 쉽게 찾아 볼 수 있는 코로나 바이러스의 전자현미경 사진들이 모두 원래는 흑백사진이라는 것을 알고 계신가요? 전자빔을 이용하는 특성상 전자현미경 사진은 흑백일 수밖에 없기 때문에, 흑백사진을 포토샵으로 이용해서 보기 편하게 채색을 하는 것입니다. 포토샵을 이용하지 않고 전자현미경 사진을 딥러닝을 이용해서 컬러링 하는 프로젝트를 진행하였습니다. \n",
        "\n",
        "사용하고자 계획했던 Cell Image Libarary data를 사용하지 못하게 되어 데이터 셋의 규모가 예상보다 작아지게 되었습니다. 데이터의 부족을 해결하기 위해서 이를 보완하기 위해서 transfer learning을 적용하였습니다. 약 40000개의 MIT place (풍경 및 건물 사진 데이터)를 이용해서 colorization 모델을 pre-training 하는 과정을 진행한 후에 현미경 이미지 데이터로 training 하는 과정을 거쳤습니다.\n",
        "\n",
        "대부분의 작업을 Colab을 사용해서 진행하였기 때문에 각 section에서 <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"center\">로 들어가면 자세한 코드를 확인 할 수 있습니다. 참고 부탁드립니다. 또한 모든 작업 내용들은 [github repository](https://github.com/YeojinKim220/DL_Project_Micrograph_Colorization)에 저장되어 있습니다. \n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tu_jGxufCuKl",
        "colab_type": "text"
      },
      "source": [
        "## Pre-training [<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"center\">](https://colab.research.google.com/drive/1J_kLfx5d0VWTJZDehBXU9_VAcVRoAPFV#scrollTo=rRoQxRmuWqnG)\n",
        "\n",
        "### Dataset \n",
        "  MIT Places (http://places.csail.mit.edu/) 데이터의 일부를 사용했고, 이 dataset은 41,000개의 풍경이나 건물 데이터를 포함하고 있습니다. 이중 1000개의 image를 validation set으로 사용하였습니다. \n",
        "![MIT Places](http://places.csail.mit.edu/img/logobanner.jpg)\n",
        "\n",
        "### Model \n",
        "  먼저 RGB Image를 scikit-image library를 이용해서 LAB image로 변환합니다. CNN 모델은 Input Image의 lightness channel인 L-channel을 input이고 이를 바탕으로 2 x H x W 의 A와 B channel을 예측하는 구조입니다. ResNet-18을 이용해서 feature를 뽑고, convolution과 upsamplling이 포함되어있는 deconvolutional layers를 이용해서 upscale하여 resolution을 높였습니다. 참고로 input image의 size는 H=224, W=224입니다. \n",
        "\n",
        "![pre-trained model](https://github.com/YeojinKim220/DL_Project_Micrograph_Colorization/blob/master/Fig.1_pre-trained_model.png?raw=true)\n",
        "\n",
        "### Output\n",
        "  L2 loss의 그래프를 통해서 epoch = 1 이후로 loss 가 급격하게 줄어들고 안정화 되는 것을 확인할 수 있습니다. 또한 validation의 loss가 증가하지 않는 것을 통해서 overfitting이 되지 않았다는 것을 확인 할 수 있습니다. \n",
        "\n",
        "![pre-trained loss](https://github.com/YeojinKim220/DL_Project_Micrograph_Colorization/blob/master/pre-training%20loss.png?raw=true)\n",
        "\n",
        "epoch에 따른 validation data의 output입니다. epoch가 증가할 수록 색의 경계가 뚜렷하게 나타나는 것을 볼 수 있습니다. \n",
        "\n",
        "![pre-trained epoch](https://github.com/YeojinKim220/DL_Project_Micrograph_Colorization/blob/master/images/pre-trained_epoch.png?raw=true)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_OiVEsWJsPW",
        "colab_type": "text"
      },
      "source": [
        "## Traning [<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"center\">](https://colab.research.google.com/drive/1AFxY4jd1_G22KS3teCVzE-RLKcHecU7w#scrollTo=Zfp5Ce54Hbyf)\n",
        "\n",
        "### Dataset \n",
        "Google Image를 이용해서 color micrograph (현미경 이미지) 4845개를 포함하는 dataset을 만들었습니다. 500개의 image를 validation set으로 이용했습니다. \n",
        "\n",
        "*참고 자료: https://github.com/sczhengyabin/Image-Downloader\n",
        "\n",
        "### Model \n",
        "모델은 pre-trained 과 동일하고 pre-trained model을 기반으로 paramteter를 update 시키는 과정을 진행했습니다.\n",
        "\n",
        "![training](https://github.com/YeojinKim220/DL_Project_Micrograph_Colorization/raw/master/Fig.2_Training_model.png?raw=true)\n",
        "\n",
        "### Output\n",
        "  L2 loss의 그래프를 통해서 모델을 pre-trained 하였기 때문에 loss가 급격하게 감소하는 부분 없이 loss가 감소하는 것을 확인 할 수 있습니다. validation의 loss가 증가하지 않는 것을 통해서 overfitting이 되지 않았다는 것을 확인 할 수 있습니다.\n",
        "  \n",
        "![trained loss](https://github.com/YeojinKim220/DL_Project_Micrograph_Colorization/blob/master/training%20loss.png?raw=true)\n",
        "\n",
        "  Epoch에 따른 결과에서 Epoch가 증가할 수록 색의 명암이 구별되는 것을 볼 수 있습니다. \n",
        "\n",
        "![trained epoch](https://github.com/YeojinKim220/DL_Project_Micrograph_Colorization/blob/master/images/trained_epoch.png?raw=true)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05w7KaEjJuh5",
        "colab_type": "text"
      },
      "source": [
        "## Result [<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"center\">](https://colab.research.google.com/drive/1H1A78ZW90MhNQgFCAFcW3S8RR1FpQanp#scrollTo=MPNK98Emmf9j)\n",
        "\n",
        "이미지 URL을 입력하면 model을 통해 colorization된 결과를 확인 할 수 있도록 Colab Notebook을 작성하였습니다. \n",
        "<img src=\"https://github.com/YeojinKim220/DL_Project_Micrograph_Colorization/blob/master/images/execute.png?raw=true\">\n",
        "<img src=\"https://github.com/YeojinKim220/DL_Project_Micrograph_Colorization/blob/master/images/example_image_2.png?raw=true\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3wcHj4rJv3Y",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Deep learning을 이용한 colorization은 초기에는 CNN으로 시작되었으나 현재는 GAN(Generative Adversarial Networks)을 사용하고 있습니다. GAN으로 training 된 모델을 사용해서 현미경 이미지를 colorization하면 정확도가 높아집니다. 하지만, Colab의 GPU runtime을 이용해서 GAN 모델을 학습 시키는 시간이 너무 오래걸리기 때문에 GAN을 사용해보지 못한것이 아쉽습니다.\n"
      ]
    }
  ]
}